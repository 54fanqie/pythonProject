{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文件为 jupyter 工程文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [官方文档](https://python.langchain.com/docs/get_started/introduction.html)\n",
    "### [入门课程](https://learn.deeplearning.ai/langchain-chat-with-your-data)\n",
    "![overview.jpg](./assets/overview.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! apt-get update\n",
    "#! apt-get install python3.10\n",
    "#! pip install torch>=2.0\n",
    "#! pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader\n",
    "LangChain 提供了多种多样的 Loader 来加载数据, 并且支持实现自定义的 \n",
    "<br>\n",
    "[预定义的 Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests\n",
    "#!pip install jq\n",
    "#!pip install unstructured\n",
    "# https://python.langchain.com/docs/modules/data_connection/document_loaders/how_to/markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import urllib3\n",
    "import os\n",
    "import os.path\n",
    "from pprint import pprint\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "def fetch(pid):\n",
    "    title = \"\"\n",
    "    content = \"\"\n",
    "    wiki_url = \"https://172.16.14.66:8889/server/index.php?s=/api/page/info\"\n",
    "    response = requests.post(wiki_url, data={ \"page_id\" : pid }, verify=False)\n",
    "    json_ret = json.loads(response.text)\n",
    "    if json_ret[\"error_code\"] == 0:\n",
    "        data = json_ret[\"data\"]\n",
    "        title = data[\"page_title\"]\n",
    "        content = data[\"page_content\"]\n",
    "    return title, content\n",
    "\n",
    "wiki_dir = \"/mnt/d/workspace/wiki\"\n",
    "\n",
    "index_content = fetch(189)[-1]\n",
    "data = []\n",
    "if len(index_content) > 0:\n",
    "    # match #### [飞腾环境下图形化界面问题](https://172.16.14.66:8889/web/#/6/23 \"飞腾环境下图形化界面问题\")\n",
    "    pattern = r\"\\((.*?)\\s+(.*?)\\)\"\n",
    "    lines = index_content.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        match = re.search(pattern, line)\n",
    "        if match:\n",
    "            url = match.group(1)\n",
    "            pid = url.rsplit(\"/\", 1)[-1]\n",
    "            title = match.group(2)[1:-1]\n",
    "            data.append((title, url, pid))\n",
    "# pprint(data)\n",
    "\n",
    "documents = []\n",
    "for tuple in data:\n",
    "    path = f\"{wiki_dir}/{tuple[-1]}.md\"\n",
    "    if not os.path.exists(path):\n",
    "        [title, content] = fetch(tuple[-1])\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(content)\n",
    "            f.close()\n",
    "\n",
    "    stat = os.stat(path)\n",
    "    if stat.st_size > 0:\n",
    "        loader = UnstructuredMarkdownLoader(path)\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "            md = doc.metadata\n",
    "            md[\"title\"] = title\n",
    "            md[\"id\"] = tuple[-1]\n",
    "        documents.extend(docs)\n",
    "\n",
    "len(documents)\n",
    "pprint(documents[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitter\n",
    "为了更好的进行处理, 模型对最大 token 的支持也不一样, 通常要把文档拆分后进行向量化. 通常是拆分成句子\n",
    "<br>\n",
    "[预定义的 Splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 document 中的文本内容\n",
    "def mapDocs(documents):\n",
    "    docs = []\n",
    "    for doc in documents:\n",
    "        docs.append(doc.page_content)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基础的也是最常用的拆分器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 5,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "txt_docs = text_splitter.split_documents(documents)\n",
    "pprint(mapDocs(txt_docs[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理代码语法的拆分器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    Language,\n",
    ")\n",
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN, chunk_size=64, chunk_overlap=0\n",
    ")\n",
    "md_docs = md_splitter.split_documents(documents)\n",
    "pprint(mapDocs(md_docs[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers\n",
    "#!apt-get install git-lfs\n",
    "#!git lfs install\n",
    "#!git clone https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型语句 token 拆分器, 需要配合向量模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "pmmb_model = \"/mnt/e/ai/models/paraphrase-multilingual-mpnet-base-v2\"\n",
    "st_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, model_name=pmmb_model)\n",
    "st_docs = st_splitter.split_documents(documents)\n",
    "pprint(mapDocs(st_docs[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "使用嵌入向量化模型将数据向量化\n",
    "<br>\n",
    "[Embedding 接口](https://python.langchain.com/docs/modules/data_connection/text_embedding/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常规的 embedding 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "# embedding_model = \"/mnt/e/ai/models/m3e-base\"\n",
    "embedding_model = \"/mnt/e/ai/models/text2vec-large-chinese\"\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=embedding_model)\n",
    "test_ret = embeddings.embed_query(\"hello, world\")\n",
    "pprint(len(test_ret))\n",
    "st_embedded_docs = embeddings.embed_documents(mapDocs(txt_docs))\n",
    "# pprint(st_embedded_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定的 embedding 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "pmmb_model = SentenceTransformer('/mnt/e/ai/models/paraphrase-multilingual-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_text0 = \"轻阅读无法启动\"\n",
    "test_text1 = \"转换服务无法启动\"\n",
    "test_text2 = \"字体安装问题\"\n",
    "tt0 = embeddings.embed_query(test_text0)\n",
    "tt1 = embeddings.embed_query(test_text1)\n",
    "tt2 = embeddings.embed_query(test_text2)\n",
    "print(np.dot(tt0, tt0))\n",
    "print(np.dot(tt0, tt1))\n",
    "print(np.dot(tt0, tt2))\n",
    "# print(np.dot(st_embedded_docs[0], st_embedded_docs[0]))\n",
    "# print(np.dot(st_embedded_docs[0], st_embedded_docs[1]))\n",
    "cos_sim = np.array(tt0).dot(tt0) / (np.linalg.norm(tt0) * np.linalg.norm(tt0))\n",
    "print(cos_sim)\n",
    "cos_sim = np.array(tt0).dot(tt1) / (np.linalg.norm(tt0) * np.linalg.norm(tt1))\n",
    "print(cos_sim)\n",
    "cos_sim = np.array(tt0).dot(tt2) / (np.linalg.norm(tt0) * np.linalg.norm(tt2))\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt0 = pmmb_model.encode(test_text0)\n",
    "tt1 = pmmb_model.encode(test_text1)\n",
    "tt2 = pmmb_model.encode(test_text2)\n",
    "print(np.dot(tt0, tt0))\n",
    "print(np.dot(tt0, tt1))\n",
    "print(np.dot(tt0, tt2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Stores\n",
    "存储并查询 embedding 的向量\n",
    "<br>\n",
    "[Store 接口](https://python.langchain.com/docs/modules/data_connection/vectorstores/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAISS, 其他诸如 Chroma 等都可以使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install faiss-gpu\n",
    "#!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_documents(txt_docs, embedding=embeddings)\n",
    "# db.save_local(\"faiss_db\")\n",
    "# db = FAISS.load_local(\"faiss_db\", embeddings=embeddings)\n",
    "search_ret = db.similarity_search_with_score(\"安装字体\", k=3)\n",
    "pprint(search_ret)\n",
    "search_ret = db.similarity_search_with_score(\"安装字体\", k=3, filter={\"id\": \"451\"})\n",
    "pprint(search_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever\n",
    "查询向量转换回数据\n",
    "<br>\n",
    "[Retriever 接口](https://python.langchain.com/docs/modules/data_connection/retrievers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!git clone https://github.com/THUDM/ChatGLM2-6B\n",
    "#!cd ChatGLM2-6B\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 ChatGLM2 LLM 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "# 最新的 162b620e 模型本地加载报错, 上一版本可以正常加载\n",
    "# 自动从 huggingface 来下来的模型存储在  ~/.cache/huggingface/ 下, 可以复制到别的机器上使用\n",
    "model_id = \"THUDM/chatglm2-6b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_id, trust_remote_code=True, device='cuda')\n",
    "model = model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要实现一个自定义的 LLM 类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/hwchase17/langchain/discussions/6969\n",
    "from typing import Any, List\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "class Glm2LLM(LLM):\n",
    "    model: object = None\n",
    "    tokenizer: object = None\n",
    "    max_token: int = 10000\n",
    "    temperature: float = 0.01\n",
    "    top_p = 0.9\n",
    "    history = []\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ChatGLM2\"\n",
    "    \n",
    "    def _call(self, prompt: str, stop: List[str] | None = None, run_manager: CallbackManagerForLLMRun | None = None, **kwargs: Any) -> str:\n",
    "        response, _ = self.model.chat(\n",
    "            self.tokenizer,\n",
    "            prompt,\n",
    "            history=self.history,\n",
    "            max_length=self.max_token,\n",
    "            temperature=self.temperature\n",
    "        )\n",
    "        self.history = self.history + [[prompt, response]]\n",
    "        return response\n",
    "\n",
    "llm = Glm2LLM(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"你是谁\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat\n",
    "由 LLM 来进行语料处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"\n",
    "{context}\n",
    "做为运维人员, 根据上述已知信息，简洁和专业的来回答用户的问题。如果无法从中得到答案，请说 “根据已知信息无法回答该问题”，不允许在答案中添加编造成分，答案请使用中文。\n",
    "问题: {question}\n",
    "\"\"\"\n",
    "qa_prompt = PromptTemplate(input_variables=[\"context\", \"question\"],template=template)\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                    #    chain_type=\"\", // stuff refine map_reduce\n",
    "                                       retriever=db.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\": qa_prompt})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#另一种方式\n",
    "def get_answer(question):\n",
    "    docs_with_score = db.similarity_search_with_score(question, k=3)\n",
    "    if len(docs_with_score) > 0:\n",
    "        # pprint(docs_with_score)\n",
    "        context = []\n",
    "        for ds in docs_with_score:\n",
    "            context.append(ds[0].page_content)\n",
    "        prompt = \"\"\"\n",
    "{context}\n",
    "做为运维人员, 根据上述已知信息，简洁和专业的来回答用户的问题。如果无法从中得到答案，请说 “根据已知信息无法回答该问题”，不允许在答案中添加编造成分，答案请使用中文。\n",
    "问题: {question}\n",
    "\"\"\".replace(\"{question}\", question).replace(\"{context}\", \"\\n\".join(context))\n",
    "        llm_ret = llm.generate([{\"prompt\": prompt, \"history\": [], \"streaming\": False}])\n",
    "        g = llm_ret.generations[0][0]\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#清理显存\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    print(f\"Emptying gpu cache {device}...\")\n",
    "    with torch.cuda.device(device):\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(qa_chain({\"query\": \"怎样安装字体?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(qa_chain({\"query\": \"转换服务如何部署?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(get_answer(\"怎样安装字体?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
