{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#清理显存\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    print(f\"Emptying gpu cache {device}...\")\n",
    "    with torch.cuda.device(device):\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = True\n",
    "\n",
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"e4751cedfe6633b2a83aa11c1a0b5659187b691af718d28f377f4190a29959d8\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://localhost:8123/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"iwillbeback\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/modules/agents/how_to/custom_llm_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain import SerpAPIWrapper, LLMChain\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish, OutputParserException\n",
    "from pprint import pprint\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载 ChatGlm2 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "# 最新的 162b620e 模型本地加载报错, 上一版本可以正常加载\n",
    "# 自动从 huggingface 来下来的模型存储在  ~/.cache/huggingface/ 下, 可以复制到别的机器上使用\n",
    "model_id = \"THUDM/chatglm2-6b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_id, trust_remote_code=True, device='cuda')\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/hwchase17/langchain/discussions/6969\n",
    "from typing import Any, List\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "class Glm2LLM(LLM):\n",
    "    model: object = None\n",
    "    tokenizer: object = None\n",
    "    max_token: int = 10000\n",
    "    temperature: float = 0.01\n",
    "    top_p = 0.9\n",
    "    history = []\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ChatGLM2\"\n",
    "    \n",
    "    def _call(self, prompt: str, stop: List[str] | None = None, run_manager: CallbackManagerForLLMRun | None = None, **kwargs: Any) -> str:\n",
    "        response, _ = self.model.chat(\n",
    "            self.tokenizer,\n",
    "            prompt,\n",
    "            history=self.history,\n",
    "            max_length=self.max_token,\n",
    "            temperature=self.temperature\n",
    "        )\n",
    "        self.history = self.history + [[prompt, response]]\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Glm2LLM(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 Serp 来进行 google 搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which tools the agent can use to answer user queries\n",
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"搜索\",\n",
    "        func=search.run,\n",
    "        description=\"执行搜索\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是官方示例, 主要针对 GPT, GPT 可以理解并很好的执行, GLM 无法理解, 会自行编造结果\n",
    "详细见: https://zhuanlan.zhihu.com/p/635724707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/modules/agents/how_to/custom_llm_agent\n",
    "\n",
    "template = \"\"\"请你回答问题. 你可以使用以下工具:\n",
    "\n",
    "{tools}\n",
    "\n",
    "使用以下格式输出:\n",
    "\n",
    "问题: 你必须回答的问题\n",
    "思考: 你的思考过程\n",
    "动作: 从工具 [{tool_names}] 中选取一项执行动作\n",
    "输入: 执行动作的输入参数\n",
    "观察: 执行动作的结果\n",
    "... (思考/动作/输入/观察 可以重复多次)\n",
    "思考: 我得到最终答案了\n",
    "答案: 问题的最终答案\n",
    "\n",
    "现在开始回答问题.\n",
    "\n",
    "问题: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    # The list of tools available\n",
    "    tools: List[Tool]\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\n观察: {observation}\\n思考: \"\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        ret = self.template.format(**kwargs)\n",
    "        print(f\"Template: {ret}\")\n",
    "        return ret\n",
    "    \n",
    "prompt = CustomPromptTemplate(\n",
    "    template=template,\n",
    "    tools=tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOutputParser(AgentOutputParser):\n",
    "\n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        print(f\"LLM output: {llm_output}\")\n",
    "        # Check if agent should finish\n",
    "        if \"答案:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.split(\"答案:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # Parse out the action and action input\n",
    "        regex = r\"动作\\s*\\d*\\s*:(.*?)\\n输入:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "\n",
    "        pprint(match)\n",
    "\n",
    "        if not match:\n",
    "            # raise OutputParserException(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        # Return the action and action input\n",
    "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "    \n",
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "# LLM chain consisting of the LLM and a prompt\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "tool_names = [tool.name for tool in tools]\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\n观察:\"],\n",
    "    allowed_tools=tool_names,\n",
    ")\n",
    "# memory = ConversationBufferWindowMemory(k=2)\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://github.com/jayli/langchain-GLM_Agent/tree/main\n",
    "glm_template = \"\"\"\n",
    "你现在是一个{role}。这里是一些已知信息：\n",
    "{background_infomation}\n",
    "{question_guide}：{input}\n",
    "\n",
    "{answer_format}\n",
    "\"\"\"\n",
    "\n",
    "glm_tools = [\n",
    "        Tool.from_function(\n",
    "            func=search.run,\n",
    "            name=\"DeepSearch\",\n",
    "            description=\"\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "class GlmCustomPromptTemplate(StringPromptTemplate):\n",
    "    template: str\n",
    "    tools: List[Tool]\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # pprint(kwargs)\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        # 没有互联网查询信息\n",
    "        if len(intermediate_steps) == 0:\n",
    "            background_infomation = \"\\n\"\n",
    "            role = \"傻瓜机器人\"\n",
    "            question_guide = \"我现在有一个问题\"\n",
    "            answer_format = \"如果你知道答案，请直接给出你的回答！如果你不知道答案，请你只回答\\\"DeepSearch('搜索词')\\\"，并将'搜索词'替换为你认为需要搜索的关键词，除此之外不要回答其他任何内容。\\n\\n下面请回答我上面提出的问题！\"\n",
    "\n",
    "        # 返回了背景信息\n",
    "        else:\n",
    "            # 根据 intermediate_steps 中的 AgentAction 拼装 background_infomation\n",
    "            background_infomation = \"\\n\\n你还有这些已知信息作为参考：\\n\\n\"\n",
    "            action, observation = intermediate_steps[0]\n",
    "            background_infomation += f\"{observation}\\n\"\n",
    "            role = \"聪明的 AI 助手\"\n",
    "            question_guide = \"请根据这些已知信息回答我的问题\"\n",
    "            answer_format = \"\"\n",
    "\n",
    "        kwargs[\"background_infomation\"] = background_infomation\n",
    "        kwargs[\"role\"] = role\n",
    "        kwargs[\"question_guide\"] = question_guide\n",
    "        kwargs[\"answer_format\"] = answer_format\n",
    "        return self.template.format(**kwargs)\n",
    "    \n",
    "glm_cpt = GlmCustomPromptTemplate(template=glm_template, tools=glm_tools, input_variables=[\"input\", \"intermediate_steps\"])\n",
    "\n",
    "class GlmCustomOutputParser(AgentOutputParser):\n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "\n",
    "        # print(f\"GlmCustomOutput: {llm_output}\")\n",
    "        # group1 = 调用函数名字\n",
    "        # group2 = 传入参数\n",
    "        match = re.match(r'^[\\s\\w]*(DeepSearch)\\(([^\\)]+)\\)', llm_output, re.DOTALL)\n",
    "\n",
    "        # 如果 llm 没有返回 DeepSearch() 则认为直接结束指令\n",
    "        if not match:\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": llm_output.strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # 否则的话都认为需要调用 Tool\n",
    "        else:\n",
    "            action = match.group(1).strip()\n",
    "            action_input = match.group(2).strip()\n",
    "            return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "\n",
    "glm_cop = GlmCustomOutputParser()\n",
    "glm_chain = LLMChain(llm=llm, prompt=glm_cpt)\n",
    "glm_agent = LLMSingleActionAgent(llm_chain=glm_chain, output_parser=glm_cop, stop=[\"\\nObservation:\"], allowed_tools=[tool.name for tool in glm_tools])\n",
    "\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=glm_agent, tools=glm_tools, verbose=True)\n",
    "# print(agent_executor.run(related_content=\"\", input=\"请问近期携程有什么大的新闻\", tool_name=\"DeepSearch\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.run(\"截至2023年7月, 金山办公的市值是多少?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.run(\"今天北京的天气怎样?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
